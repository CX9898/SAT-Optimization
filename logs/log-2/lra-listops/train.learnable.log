GPU list: [0, 1, 2, 3]
[
    {
        "learn_pos_emb": true,
        "tied_weights": false,
        "embedding_dim": 64,
        "transformer_dim": 64,
        "transformer_hidden_dim": 128,
        "head_dim": 32,
        "num_head": 2,
        "num_layers": 4,
        "vocab_size": 32,
        "max_seq_len": 2048,
        "dropout_prob": 0.1,
        "attention_dropout": 0.1,
        "pooling_mode": "MEAN",
        "num_classes": 10,
        "block_size": 64,
        "batch_size": 32,
        "density": 0.04,
        "mixed_precision": true,
        "random_seed": 2,
        "task": "lra-listops"
    },
    {
        "batch_size": 128,
        "learning_rate": 0.0006,
        "warmup": 1000,
        "lr_decay": "linear",
        "weight_decay": 0,
        "eval_frequency": 500,
        "num_train_steps": 10000,
        "num_init_steps": 1000,
        "num_eval_steps": 62,
        "num_dense_train_steps": 1000,
        "patience": 10,
        "attn_loss_scale": 0.01
    }
]
attn_mask compile
attn_mask compile
attn_mask compile
attn_mask compile
DataParallel(
  (module): ModelForSC(
    (model): Model(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(32, 64)
        (position_embeddings): Embedding(2048, 64)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_0): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_1): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_2): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_3): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (seq_classifer): SCHead(
      (mlpblock): Sequential(
        (0): Linear(in_features=64, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=10, bias=True)
      )
    )
  )
)
parameter_size: [torch.Size([32, 64]), torch.Size([2048, 64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]
num_parameter: 280842
Loaded ../Skyformer/data/lra_processed/lra-listops.train.pickle... size=96000
Loaded ../Skyformer/data/lra_processed/lra-listops.dev.pickle... size=2000
Loaded ../Skyformer/data/lra_processed/lra-listops.test.pickle... size=2000
accumu_steps=1
[tensor([34.6000, 34.6000, 34.6000, 34.6000], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([33.6353, 33.6353, 33.6353, 33.6353], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([38.2356, 38.2355, 38.2356, 38.2356], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([32.0813, 32.0813, 32.0812, 32.0813], device='cuda:0',
       grad_fn=<GatherBackward>)]
best model saved: step =  499 dev accu =  tensor(0.1711, device='cuda:0')

Validation Results
Global Steps: 499
Valid Loss: 2.25937
Valid Accuracy: 0.17112
time stamp: 828.0679578781128
[tensor([9.4267, 9.4267, 9.4267, 9.4267], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([9.1671, 9.1671, 9.1671, 9.1671], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([10.6220, 10.6220, 10.6220, 10.6220], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([8.6879, 8.6879, 8.6879, 8.6878], device='cuda:0',
       grad_fn=<GatherBackward>)]
best model saved: step =  999 dev accu =  tensor(0.3577, device='cuda:0')

Validation Results
Global Steps: 999
Valid Loss: 1.73795
Valid Accuracy: 0.35774
time stamp: 1656.1255717277527
module.model.transformer_0.mha.pattern saved
module.model.transformer_1.mha.pattern saved
module.model.transformer_2.mha.pattern saved
module.model.transformer_3.mha.pattern saved
./pickle/lra-listops/module.model.transformer_0.mha.pattern.pickle
tensor([ 924,   66,   99,   33,  297,  990,  462,  495,  759,  561,  132,  231,
         429,  528,  396,  693,  792,  165,  264, 1023,    0,  660,  627,  891,
         825,  594,  330,  858,  726,  316,  905,  363,   78,  450,  446,  973,
         926,  988,  957,  122], device='cuda:0')
tensor(40, device='cuda:0')
block_attn_mask compile
./pickle/lra-listops/module.model.transformer_1.mha.pattern.pickle
tensor([  33,  231,  990,  759,  660,  858,  165,  132,  264,  396,  726,  825,
         495,   66,  462,  429,  594,   39,  225,  198,  957,  891,  330,  297,
         561, 1023,  363,  693,  792,  244,  647,   52,  641,   99,  148,  644,
         157,  932,  146,  580], device='cuda:0')
tensor(40, device='cuda:0')
block_attn_mask compile
./pickle/lra-listops/module.model.transformer_2.mha.pattern.pickle
tensor([ 462,  528,  726,  132,  858,  165,  198,  330,  990,  594,   99,  297,
         891,  363,  429,  693,  627,  759, 1023,  957,  495,    0,  231,  396,
         561,  660,  142,  452,   33,  539,  880,  406,  716,  264,  206,  454,
         144,  516,   66,  154], device='cuda:0')
tensor(40, device='cuda:0')
block_attn_mask compile
./pickle/lra-listops/module.model.transformer_3.mha.pattern.pickle
tensor([198, 363,   0, 429, 957, 396, 924,  33,  66, 528, 165, 231, 264, 330,
        297, 132, 561, 726, 627, 990, 825, 660,  99, 792, 495, 693,   6, 192,
        891, 204, 390, 858,  29, 928, 275, 616,  34,  65, 285, 936],
       device='cuda:0')
tensor(40, device='cuda:0')
block_attn_mask compile
total pattern searching time (s): 0.12320971488952637
[]
best model saved: step =  1499 dev accu =  tensor(0.3633, device='cuda:0')

Validation Results
Global Steps: 1499
Valid Loss: 1.74259
Valid Accuracy: 0.36328
time stamp: 1862.7482676506042
[]

Validation Results
Global Steps: 1999
Valid Loss: 1.73873
Valid Accuracy: 0.35736
time stamp: 1908.6106498241425
[]

Validation Results
Global Steps: 2499
Valid Loss: 1.72077
Valid Accuracy: 0.34904
time stamp: 1954.4464359283447
[]

Validation Results
Global Steps: 2999
Valid Loss: 1.69897
Valid Accuracy: 0.36164
time stamp: 2000.753000497818
[]

Validation Results
Global Steps: 3499
Valid Loss: 1.67889
Valid Accuracy: 0.35988
time stamp: 2046.798213481903
[]
best model saved: step =  3999 dev accu =  tensor(0.3667, device='cuda:0')

Validation Results
Global Steps: 3999
Valid Loss: 1.65439
Valid Accuracy: 0.36668
time stamp: 2092.871921777725
[]

Validation Results
Global Steps: 4499
Valid Loss: 1.66436
Valid Accuracy: 0.36265
time stamp: 2138.2538130283356
[]

Validation Results
Global Steps: 4999
Valid Loss: 1.65436
Valid Accuracy: 0.35799
time stamp: 2184.0684020519257
[]

Validation Results
Global Steps: 5499
Valid Loss: 1.65599
Valid Accuracy: 0.36492
time stamp: 2230.3018004894257
[]

Validation Results
Global Steps: 5999
Valid Loss: 1.64711
Valid Accuracy: 0.36580
time stamp: 2276.064670562744
[]
best model saved: step =  6499 dev accu =  tensor(0.3708, device='cuda:0')

Validation Results
Global Steps: 6499
Valid Loss: 1.64634
Valid Accuracy: 0.37084
time stamp: 2321.813328742981
[]

Validation Results
Global Steps: 6999
Valid Loss: 1.65037
Valid Accuracy: 0.36845
time stamp: 2366.3854694366455
[]
best model saved: step =  7499 dev accu =  tensor(0.3720, device='cuda:0')

Validation Results
Global Steps: 7499
Valid Loss: 1.64743
Valid Accuracy: 0.37198
time stamp: 2411.483532190323
[]
best model saved: step =  7999 dev accu =  tensor(0.3732, device='cuda:0')

Validation Results
Global Steps: 7999
Valid Loss: 1.63885
Valid Accuracy: 0.37324
time stamp: 2455.446002483368
[]

Validation Results
Global Steps: 8499
Valid Loss: 1.64672
Valid Accuracy: 0.37223
time stamp: 2499.172380208969
[]

Validation Results
Global Steps: 8999
Valid Loss: 1.64873
Valid Accuracy: 0.36769
time stamp: 2543.625365972519
[]

Validation Results
Global Steps: 9499
Valid Loss: 1.64614
Valid Accuracy: 0.37122
time stamp: 2587.626139163971
[]

Validation Results
Global Steps: 9999
Valid Loss: 1.64867
Valid Accuracy: 0.37034
time stamp: 2631.2931644916534
total training step (k): 10.0
total training time (s): 2631.2936091423035
total training time (ms): 53560.366455078125
peak memory usage (MB): 12399
allocated memory usage (MB): 83140202
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    7572 KB |   12399 MB |   81191 GB |   81191 GB |
|       from large pool |    3145 KB |   12391 MB |   80684 GB |   80684 GB |
|       from small pool |    4427 KB |      13 MB |     507 GB |     507 GB |
|---------------------------------------------------------------------------|
| Active memory         |    7572 KB |   12399 MB |   81191 GB |   81191 GB |
|       from large pool |    3145 KB |   12391 MB |   80684 GB |   80684 GB |
|       from small pool |    4427 KB |      13 MB |     507 GB |     507 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   13530 MB |   13530 MB |   13530 MB |       0 B  |
|       from large pool |   13516 MB |   13516 MB |   13516 MB |       0 B  |
|       from small pool |      14 MB |      14 MB |      14 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1040 MB |    2061 MB |   43064 GB |   43063 GB |
|       from large pool |    1036 MB |    2056 MB |   42516 GB |   42515 GB |
|       from small pool |       3 MB |       7 MB |     548 GB |     548 GB |
|---------------------------------------------------------------------------|
| Allocations           |     246    |     407    |    8634 K  |    8634 K  |
|       from large pool |       2    |      93    |    3520 K  |    3520 K  |
|       from small pool |     244    |     381    |    5113 K  |    5113 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     246    |     407    |    8634 K  |    8634 K  |
|       from large pool |       2    |      93    |    3520 K  |    3520 K  |
|       from small pool |     244    |     381    |    5113 K  |    5113 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      32    |      32    |      32    |       0    |
|       from large pool |      25    |      25    |      25    |       0    |
|       from small pool |       7    |       7    |       7    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      31    |    4395 K  |    4395 K  |
|       from large pool |       3    |      18    |    1863 K  |    1863 K  |
|       from small pool |       7    |      18    |    2532 K  |    2532 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

loading the best model from: ./checkpoints/checkpoints-2/lra-listops/learnable.model
Evaluation Results
Loss: 1.62867
Accuracy: 0.37031
