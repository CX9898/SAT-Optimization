GPU list: [0, 1, 2, 3]
[
    {
        "learn_pos_emb": true,
        "tied_weights": false,
        "embedding_dim": 64,
        "transformer_dim": 64,
        "transformer_hidden_dim": 128,
        "head_dim": 32,
        "num_head": 2,
        "num_layers": 2,
        "vocab_size": 256,
        "max_seq_len": 1024,
        "dropout_prob": 0.1,
        "attention_dropout": 0.1,
        "pooling_mode": "MEAN",
        "num_classes": 10,
        "block_size": 32,
        "batch_size": 128,
        "density": 0.05,
        "mixed_precision": true,
        "random_seed": 4,
        "task": "lra-image"
    },
    {
        "batch_size": 512,
        "learning_rate": 0.002,
        "warmup": 175,
        "lr_decay": "linear",
        "weight_decay": 0,
        "eval_frequency": 500,
        "num_train_steps": 10000,
        "num_init_steps": 0,
        "num_eval_steps": 20,
        "num_dense_train_steps": 1000,
        "attn_loss_scale": 0.01
    }
]
attn compile
attn compile
DataParallel(
  (module): ModelForSC(
    (model): Model(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(256, 64)
        (position_embeddings): Embedding(1024, 64)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_0): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=32, stride=32, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_1): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=32, stride=32, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (seq_classifer): SCHead(
      (mlpblock): Sequential(
        (0): Linear(in_features=64, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=10, bias=True)
      )
    )
  )
)
parameter_size: [torch.Size([256, 64]), torch.Size([1024, 64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]
num_parameter: 160650
Loaded ../Skyformer/data/lra_processed/lra-image.train.pickle... size=45000
Loaded ../Skyformer/data/lra_processed/lra-image.dev.pickle... size=5000
Loaded ../Skyformer/data/lra_processed/lra-image.test.pickle... size=10000
accumu_steps=1
module.model.transformer_0.mha.pattern saved
module.model.transformer_1.mha.pattern saved
./pickle/lra-image/module.model.transformer_0.mha.pattern.pickle
tensor([198, 726, 693, 429, 594, 132, 199, 230, 214, 710, 231, 561, 396, 165,
        825,   0,  66, 216, 774, 213, 678, 660, 598, 722, 150, 708, 210, 582,
        330, 792, 891, 729, 822, 597, 690, 528, 495,   6, 192, 217, 806, 204,
        390, 205, 422, 149, 676, 297, 694, 725, 437], device='cuda:0')
tensor(51, device='cuda:0')
block_attn compile
./pickle/lra-image/module.model.transformer_1.mha.pattern.pickle
tensor([726, 627, 660,  66, 528, 792,  99, 363, 495, 264, 198, 429, 594, 165,
        990, 957, 462, 231,  86, 706, 891,  84, 642, 396,  72, 258, 536, 784,
        733, 950, 632, 787, 278, 712, 924, 272, 520,   0, 534, 720,  93, 930,
        118, 707, 502, 719,  88, 770, 728, 790, 182], device='cuda:0')
tensor(51, device='cuda:0')
block_attn compile
total pattern searching time (s): 0.005484342575073242
[]
best model saved: step =  499 dev accu =  tensor(0.3367, device='cuda:0')

Validation Results
Global Steps: 499
Valid Loss: 1.80716
Valid Accuracy: 0.33672
time stamp: 84.02819323539734
[]
best model saved: step =  999 dev accu =  tensor(0.4041, device='cuda:0')

Validation Results
Global Steps: 999
Valid Loss: 1.68540
Valid Accuracy: 0.40410
time stamp: 119.40331149101257
[]

Validation Results
Global Steps: 1499
Valid Loss: 1.67883
Valid Accuracy: 0.39590
time stamp: 154.92047381401062
[]
best model saved: step =  1999 dev accu =  tensor(0.4282, device='cuda:0')

Validation Results
Global Steps: 1999
Valid Loss: 1.64915
Valid Accuracy: 0.42822
time stamp: 190.7089638710022
[]

Validation Results
Global Steps: 2499
Valid Loss: 1.73014
Valid Accuracy: 0.41064
time stamp: 226.19465923309326
[]
best model saved: step =  2999 dev accu =  tensor(0.4286, device='cuda:0')

Validation Results
Global Steps: 2999
Valid Loss: 1.69714
Valid Accuracy: 0.42861
time stamp: 261.71789622306824
[]

Validation Results
Global Steps: 3499
Valid Loss: 1.78131
Valid Accuracy: 0.40703
time stamp: 297.41131687164307
[]

Validation Results
Global Steps: 3999
Valid Loss: 1.83452
Valid Accuracy: 0.41504
time stamp: 333.1057040691376
[]

Validation Results
Global Steps: 4499
Valid Loss: 1.82017
Valid Accuracy: 0.41338
time stamp: 368.4529573917389
[]

Validation Results
Global Steps: 4999
Valid Loss: 1.80337
Valid Accuracy: 0.42695
time stamp: 403.7822437286377
[]

Validation Results
Global Steps: 5499
Valid Loss: 1.88404
Valid Accuracy: 0.41367
time stamp: 439.2372832298279
[]

Validation Results
Global Steps: 5999
Valid Loss: 1.87037
Valid Accuracy: 0.42490
time stamp: 474.6253228187561
[]

Validation Results
Global Steps: 6499
Valid Loss: 1.91003
Valid Accuracy: 0.42168
time stamp: 510.436151266098
[]

Validation Results
Global Steps: 6999
Valid Loss: 1.93788
Valid Accuracy: 0.42705
time stamp: 546.1546332836151
[]

Validation Results
Global Steps: 7499
Valid Loss: 2.00047
Valid Accuracy: 0.41865
time stamp: 581.8928785324097
[]

Validation Results
Global Steps: 7999
Valid Loss: 2.03694
Valid Accuracy: 0.41182
time stamp: 617.5519902706146
[]

Validation Results
Global Steps: 8499
Valid Loss: 2.04384
Valid Accuracy: 0.41299
time stamp: 653.1964004039764
[]

Validation Results
Global Steps: 8999
Valid Loss: 2.07930
Valid Accuracy: 0.42012
time stamp: 688.7591516971588
[]

Validation Results
Global Steps: 9499
Valid Loss: 2.04709
Valid Accuracy: 0.42773
time stamp: 724.1471405029297
[]

Validation Results
Global Steps: 9999
Valid Loss: 2.11225
Valid Accuracy: 0.41572
time stamp: 759.5745708942413
total training step (k): 10.0
total training time (s): 759.5752551555634
total training time (ms): 13142.168762207031
peak memory usage (MB): 8036
allocated memory usage (MB): 48698888
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9993 KB |    8036 MB |   47557 GB |   47557 GB |
|       from large pool |    6144 KB |    8030 MB |   47193 GB |   47193 GB |
|       from small pool |    3849 KB |      15 MB |     364 GB |     364 GB |
|---------------------------------------------------------------------------|
| Active memory         |    9993 KB |    8036 MB |   47557 GB |   47557 GB |
|       from large pool |    6144 KB |    8030 MB |   47193 GB |   47193 GB |
|       from small pool |    3849 KB |      15 MB |     364 GB |     364 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    8612 MB |    8612 MB |    8612 MB |       0 B  |
|       from large pool |    8596 MB |    8596 MB |    8596 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   10487 KB |    1109 MB |   40826 GB |   40826 GB |
|       from large pool |   10240 KB |    1106 MB |   40375 GB |   40375 GB |
|       from small pool |     247 KB |       4 MB |     451 GB |     451 GB |
|---------------------------------------------------------------------------|
| Allocations           |     228    |     304    |    5153 K  |    5153 K  |
|       from large pool |       2    |      49    |    1786 K  |    1786 K  |
|       from small pool |     226    |     285    |    3367 K  |    3367 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     228    |     304    |    5153 K  |    5153 K  |
|       from large pool |       2    |      49    |    1786 K  |    1786 K  |
|       from small pool |     226    |     285    |    3367 K  |    3367 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      30    |      30    |      30    |       0    |
|       from large pool |      22    |      22    |      22    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      23    |    2697 K  |    2697 K  |
|       from large pool |       2    |      15    |     660 K  |     660 K  |
|       from small pool |       8    |      16    |    2036 K  |    2036 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

loading the best model from: ./checkpoints/checkpoints-4/lra-image/learnable.model
Evaluation Results
Loss: 1.70121
Accuracy: 0.42455
