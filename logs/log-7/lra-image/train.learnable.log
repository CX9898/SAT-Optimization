GPU list: [0, 1, 2, 3]
[
    {
        "learn_pos_emb": true,
        "tied_weights": false,
        "embedding_dim": 64,
        "transformer_dim": 64,
        "transformer_hidden_dim": 128,
        "head_dim": 32,
        "num_head": 2,
        "num_layers": 2,
        "vocab_size": 256,
        "max_seq_len": 1024,
        "dropout_prob": 0.1,
        "attention_dropout": 0.1,
        "pooling_mode": "MEAN",
        "num_classes": 10,
        "block_size": 32,
        "batch_size": 128,
        "density": 0.05,
        "mixed_precision": true,
        "random_seed": 7,
        "task": "lra-image"
    },
    {
        "batch_size": 512,
        "learning_rate": 0.002,
        "warmup": 175,
        "lr_decay": "linear",
        "weight_decay": 0,
        "eval_frequency": 500,
        "num_train_steps": 10000,
        "num_init_steps": 0,
        "num_eval_steps": 20,
        "num_dense_train_steps": 1000,
        "attn_loss_scale": 0.01
    }
]
attn compile
attn compile
DataParallel(
  (module): ModelForSC(
    (model): Model(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(256, 64)
        (position_embeddings): Embedding(1024, 64)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_0): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=32, stride=32, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_1): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=32, stride=32, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (seq_classifer): SCHead(
      (mlpblock): Sequential(
        (0): Linear(in_features=64, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=10, bias=True)
      )
    )
  )
)
parameter_size: [torch.Size([256, 64]), torch.Size([1024, 64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]
num_parameter: 160650
Loaded ../Skyformer/data/lra_processed/lra-image.train.pickle... size=45000
Loaded ../Skyformer/data/lra_processed/lra-image.dev.pickle... size=5000
Loaded ../Skyformer/data/lra_processed/lra-image.test.pickle... size=10000
accumu_steps=1
module.model.transformer_0.mha.pattern saved
module.model.transformer_1.mha.pattern saved
./pickle/lra-image/module.model.transformer_0.mha.pattern.pickle
tensor([ 198,  726,   66, 1023,   99,  495,  330,  363,  132,  627,  891,  957,
         528,  462,  660,  924,  693,  561,  396,   95,  994,    0,  165,   79,
         482,  231,   70,  194,  111,  483,  223,  998,  759,  733,  950,  264,
          93,  930,  139,  356,  700,  917,  594,   91,  866,  221,  934,   75,
         354,  497,  559], device='cuda:0')
tensor(51, device='cuda:0')
block_attn compile
./pickle/lra-image/module.model.transformer_1.mha.pattern.pickle
tensor([858, 132, 759, 693, 231, 330,  66,  99, 627, 792, 495, 561, 924, 891,
        429, 594, 297, 528, 762, 855,   0, 165, 726, 462, 396, 660, 151, 740,
         71, 226,  90, 834, 146, 580, 695, 757, 699, 885, 764, 919,  87, 738,
        309, 681, 120, 771, 264, 700, 917, 135, 228], device='cuda:0')
tensor(51, device='cuda:0')
block_attn compile
total pattern searching time (s): 0.00538325309753418
[]
best model saved: step =  499 dev accu =  tensor(0.3510, device='cuda:0')

Validation Results
Global Steps: 499
Valid Loss: 1.78654
Valid Accuracy: 0.35098
time stamp: 93.76700472831726
[]
best model saved: step =  999 dev accu =  tensor(0.3976, device='cuda:0')

Validation Results
Global Steps: 999
Valid Loss: 1.68424
Valid Accuracy: 0.39756
time stamp: 129.171160697937
[]
best model saved: step =  1499 dev accu =  tensor(0.3988, device='cuda:0')

Validation Results
Global Steps: 1499
Valid Loss: 1.73963
Valid Accuracy: 0.39883
time stamp: 164.48393559455872
[]
best model saved: step =  1999 dev accu =  tensor(0.4121, device='cuda:0')

Validation Results
Global Steps: 1999
Valid Loss: 1.71074
Valid Accuracy: 0.41211
time stamp: 200.28769278526306
[]
best model saved: step =  2499 dev accu =  tensor(0.4231, device='cuda:0')

Validation Results
Global Steps: 2499
Valid Loss: 1.71255
Valid Accuracy: 0.42314
time stamp: 235.96008133888245
[]
best model saved: step =  2999 dev accu =  tensor(0.4252, device='cuda:0')

Validation Results
Global Steps: 2999
Valid Loss: 1.74041
Valid Accuracy: 0.42520
time stamp: 271.3946523666382
[]
best model saved: step =  3499 dev accu =  tensor(0.4260, device='cuda:0')

Validation Results
Global Steps: 3499
Valid Loss: 1.77162
Valid Accuracy: 0.42598
time stamp: 307.08274483680725
[]

Validation Results
Global Steps: 3999
Valid Loss: 1.79982
Valid Accuracy: 0.41904
time stamp: 342.45789861679077
[]

Validation Results
Global Steps: 4499
Valid Loss: 1.83408
Valid Accuracy: 0.42432
time stamp: 377.8708691596985
[]

Validation Results
Global Steps: 4999
Valid Loss: 1.83760
Valid Accuracy: 0.41689
time stamp: 413.4179232120514
[]

Validation Results
Global Steps: 5499
Valid Loss: 1.89290
Valid Accuracy: 0.41719
time stamp: 448.99104404449463
[]

Validation Results
Global Steps: 5999
Valid Loss: 1.92658
Valid Accuracy: 0.41602
time stamp: 484.5619864463806
[]

Validation Results
Global Steps: 6499
Valid Loss: 1.96522
Valid Accuracy: 0.41963
time stamp: 520.2249808311462
[]
best model saved: step =  6999 dev accu =  tensor(0.4284, device='cuda:0')

Validation Results
Global Steps: 6999
Valid Loss: 1.94892
Valid Accuracy: 0.42842
time stamp: 555.977641582489
[]

Validation Results
Global Steps: 7499
Valid Loss: 2.11938
Valid Accuracy: 0.40918
time stamp: 591.3601186275482
[]

Validation Results
Global Steps: 7999
Valid Loss: 2.06138
Valid Accuracy: 0.41436
time stamp: 626.9320425987244
[]

Validation Results
Global Steps: 8499
Valid Loss: 2.10214
Valid Accuracy: 0.42373
time stamp: 662.8111393451691
[]

Validation Results
Global Steps: 8999
Valid Loss: 2.07675
Valid Accuracy: 0.42285
time stamp: 698.5306310653687
[]

Validation Results
Global Steps: 9499
Valid Loss: 2.11289
Valid Accuracy: 0.42607
time stamp: 734.0098993778229
[]

Validation Results
Global Steps: 9999
Valid Loss: 2.15265
Valid Accuracy: 0.41328
time stamp: 769.7742671966553
total training step (k): 10.0
total training time (s): 769.774825334549
total training time (ms): 13051.380798339844
peak memory usage (MB): 8036
allocated memory usage (MB): 49093772
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9993 KB |    8036 MB |   47943 GB |   47943 GB |
|       from large pool |    6144 KB |    8030 MB |   47578 GB |   47578 GB |
|       from small pool |    3849 KB |      15 MB |     364 GB |     364 GB |
|---------------------------------------------------------------------------|
| Active memory         |    9993 KB |    8036 MB |   47943 GB |   47943 GB |
|       from large pool |    6144 KB |    8030 MB |   47578 GB |   47578 GB |
|       from small pool |    3849 KB |      15 MB |     364 GB |     364 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    8612 MB |    8612 MB |    8612 MB |       0 B  |
|       from large pool |    8596 MB |    8596 MB |    8596 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   14583 KB |    1109 MB |   40316 GB |   40316 GB |
|       from large pool |   14336 KB |    1106 MB |   39862 GB |   39862 GB |
|       from small pool |     247 KB |       4 MB |     453 GB |     453 GB |
|---------------------------------------------------------------------------|
| Allocations           |     228    |     304    |    5153 K  |    5153 K  |
|       from large pool |       2    |      49    |    1786 K  |    1786 K  |
|       from small pool |     226    |     285    |    3367 K  |    3367 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     228    |     304    |    5153 K  |    5153 K  |
|       from large pool |       2    |      49    |    1786 K  |    1786 K  |
|       from small pool |     226    |     285    |    3367 K  |    3367 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      30    |      30    |      30    |       0    |
|       from large pool |      22    |      22    |      22    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       7    |      23    |    2524 K  |    2524 K  |
|       from large pool |       2    |      15    |     599 K  |     599 K  |
|       from small pool |       5    |      16    |    1925 K  |    1925 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

loading the best model from: ./checkpoints/checkpoints-7/lra-image/learnable.model
Evaluation Results
Loss: 1.92432
Accuracy: 0.42763
