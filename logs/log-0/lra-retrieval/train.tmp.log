GPU list: [0, 1, 2, 3]
[
    {
        "learn_pos_emb": true,
        "tied_weights": false,
        "embedding_dim": 64,
        "transformer_dim": 64,
        "transformer_hidden_dim": 128,
        "head_dim": 32,
        "num_head": 2,
        "num_layers": 2,
        "vocab_size": 512,
        "max_seq_len": 4096,
        "dropout_prob": 0.1,
        "attention_dropout": 0.1,
        "pooling_mode": "MEAN",
        "block_size": 64,
        "num_classes": 2,
        "batch_size": 8,
        "density": 0.021,
        "mixed_precision": true,
        "random_seed": 0,
        "task": "lra-retrieval"
    },
    {
        "batch_size": 32,
        "learning_rate": 0.0005,
        "warmup": 800,
        "lr_decay": "linear",
        "weight_decay": 0,
        "eval_frequency": 200,
        "num_train_steps": 40000,
        "num_init_steps": 3000,
        "num_eval_steps": 300,
        "num_dense_train_steps": 100,
        "patience": 10,
        "attn_loss_scale": 0.01
    }
]
attn_mask compile
attn_mask compile
DataParallel(
  (module): ModelForSCDual(
    (model): Model(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(512, 64)
        (position_embeddings): Embedding(4096, 64)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_0): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_1): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (seq_classifer): SCHeadDual(
      (mlpblock): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=2, bias=True)
      )
    )
  )
)
parameter_size: [torch.Size([512, 64]), torch.Size([4096, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([64, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 256]), torch.Size([128]), torch.Size([2, 128]), torch.Size([2])]
num_parameter: 399234
Loaded ../Skyformer/data/lra_processed/lra-retrieval.train.pickle... size=147086
Loaded ../Skyformer/data/lra_processed/lra-retrieval.dev.pickle... size=18090
Loaded ../Skyformer/data/lra_processed/lra-retrieval.test.pickle... size=17437
accumu_steps=1
module.model.transformer_0.mha.pattern saved
module.model.transformer_1.mha.pattern saved
./pickle/lra-retrieval/module.model.transformer_0.mha.pattern.pickle
tensor([3185,  455,  195, 3965, 3510, 3835, 3120, 2340, 2405, 1625,  497, 3143,
        3575,  130,  199,  451, 3380,  502, 3463,  585,  507, 3783, 2600, 3517,
        3958, 3445,  496, 3079, 3195, 3825,  520, 2080,  241, 3139,  975, 2665,
         488, 2567,  509, 3911, 2210, 2145, 3190, 3505,  251, 3779, 3126, 3504,
         650,  503, 3527, 3121, 3184, 2275, 1885, 3197, 3953, 2417, 3173, 1661,
        3929,  485, 2375,  232, 2563, 3837, 3963, 1021, 3919, 3770,  480, 2055,
         229, 2371, 3515, 3830,  484, 2311, 2166, 3489, 1495,  457,  583, 3191,
        3569,  477], device='cuda:0')
tensor(86, device='cuda:0')
block_attn_mask compile
./pickle/lra-retrieval/module.model.transformer_1.mha.pattern.pickle
tensor([ 910, 3445,  585, 2730, 2470, 2340, 4030, 3575, 2665, 3315, 4095, 1300,
        1105, 1170, 1950,  650, 2210, 3055, 1235, 3965, 2485, 3430, 1040, 1365,
         951, 3534, 2145, 1755,  934, 2446, 1194, 2706, 2357, 3428,  195, 1820,
        2487, 3558, 3120,  130, 3447, 3573,  937, 2638,  949, 3406,  520, 1430,
        2479, 3046,  958, 3982, 2494, 4006, 2080, 2474, 2726,  959, 4046, 1560,
        2750, 4010,  629, 3401, 2015, 3455, 4085, 1982, 3998,  676, 2314, 1962,
        2718, 1077, 3408, 1190, 2450,  932, 2318, 2359, 3556,  915, 1230,  693,
        3402,  917], device='cuda:0')
tensor(86, device='cuda:0')
block_attn_mask compile
total pattern searching time (s): 0.11689448356628418
