GPU list: [0]
[
    {
        "learn_pos_emb": true,
        "tied_weights": false,
        "embedding_dim": 64,
        "transformer_dim": 64,
        "transformer_hidden_dim": 128,
        "head_dim": 32,
        "num_head": 2,
        "num_layers": 2,
        "vocab_size": 256,
        "max_seq_len": 1024,
        "dropout_prob": 0.1,
        "attention_dropout": 0.1,
        "pooling_mode": "MEAN",
        "num_classes": 10,
        "block_size": 32,
        "batch_size": 128,
        "density": 0.05,
        "mixed_precision": true,
        "random_seed": 0,
        "task": "lra-image"
    },
    {
        "batch_size": 128,
        "learning_rate": 0.002,
        "warmup": 175,
        "lr_decay": "linear",
        "weight_decay": 0,
        "eval_frequency": 500,
        "num_train_steps": 10000,
        "num_init_steps": 0,
        "num_eval_steps": 20,
        "num_dense_train_steps": 10,
        "attn_loss_scale": 0.01,
        "skewness": 1.7,
        "distance": 1.3
    }
]
attn compile
attn compile
DataParallel(
  (module): ModelForSC(
    (model): Model(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(256, 64)
        (position_embeddings): Embedding(1024, 64)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_0): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=32, stride=32, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_1): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=32, stride=32, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (seq_classifer): SCHead(
      (mlpblock): Sequential(
        (0): Linear(in_features=64, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=10, bias=True)
      )
    )
  )
)
parameter_size: [torch.Size([256, 64]), torch.Size([1024, 64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]
num_parameter: 160650
Loaded ../Skyformer/data/lra_processed/lra-image.train.pickle... size=45000
Loaded ../Skyformer/data/lra_processed/lra-image.dev.pickle... size=5000
Loaded ../Skyformer/data/lra_processed/lra-image.test.pickle... size=10000
accumu_steps=1
module.model.transformer_0.mha.pattern saved
module.model.transformer_1.mha.pattern saved
./pickle/lra-image/module.model.transformer_0.mha.pattern.pickle
tensor([  0, 594, 858, 891, 792,  33, 363, 165,   8, 256, 264,   1,  32, 231,
         27, 864, 957,   5, 160, 330,  99,  18, 576, 603, 882, 825,  40, 257,
        561, 726, 660, 283, 872,  66, 924,   7, 224, 274, 584, 990,  29, 928,
          6, 192, 600, 786, 794, 856, 370, 587, 602], device='cuda:0')
tensor(51, device='cuda:0')
block_attn compile
./pickle/lra-image/module.model.transformer_1.mha.pattern.pickle
tensor([ 693,  528,  825,  462,  429,   66,  792,  759,   99,    0,  132,  363,
         330,  660,  924,  264,  990, 1023,  627,  469,  686,  464,  526,  495,
         661,  692,  695,  757,   85,  674,  297,  957,  436,  653,  561,  366,
         459,  341,  682,  396,  198,   33,  696,  789,   10,  320,  441,  813,
          89,  802,  437], device='cuda:0')
tensor(51, device='cuda:0')
block_attn compile
total pattern searching time (s): 0.0051305294036865234
total training step (k): 10.0
total training time (s): 58.57513451576233
total training time (ms): 0
peak memory usage (MB): 6988
allocated memory usage (MB): 4681184
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    2077 MB |    6988 MB |    4571 GB |    4569 GB |
|       from large pool |    2072 MB |    6976 MB |    4559 GB |    4557 GB |
|       from small pool |       5 MB |      19 MB |      11 GB |      11 GB |
|---------------------------------------------------------------------------|
| Active memory         |    2077 MB |    6988 MB |    4571 GB |    4569 GB |
|       from large pool |    2072 MB |    6976 MB |    4559 GB |    4557 GB |
|       from small pool |       5 MB |      19 MB |      11 GB |      11 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    7592 MB |    7592 MB |    7592 MB |       0 B  |
|       from large pool |    7572 MB |    7572 MB |    7572 MB |       0 B  |
|       from small pool |      20 MB |      20 MB |      20 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1056 MB |    1105 MB |  736179 MB |  735122 MB |
|       from large pool |    1052 MB |    1100 MB |  723405 MB |  722353 MB |
|       from small pool |       4 MB |      10 MB |   12773 MB |   12769 MB |
|---------------------------------------------------------------------------|
| Allocations           |     274    |     363    |  130450    |  130176    |
|       from large pool |       6    |      46    |   46620    |   46614    |
|       from small pool |     268    |     320    |   83830    |   83562    |
|---------------------------------------------------------------------------|
| Active allocs         |     274    |     363    |  130450    |  130176    |
|       from large pool |       6    |      46    |   46620    |   46614    |
|       from small pool |     268    |     320    |   83830    |   83562    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      31    |      31    |      31    |       0    |
|       from large pool |      21    |      21    |      21    |       0    |
|       from small pool |      10    |      10    |      10    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      14    |      27    |   69023    |   69009    |
|       from large pool |       5    |      10    |   18704    |   18699    |
|       from small pool |       9    |      21    |   50319    |   50310    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

dense memory(MB) : {} 4622809
dense step : {} 254
dense time : {} 57.75025939941406
sparse memory(MB) : {} 4678686
sparse step : {} 10
sparse_time : {} 1714021334.0493453
loading the best model from: ./checkpoints/checkpoints-0/lra-image/tmp.model
