GPU list: [0, 1, 2, 3]
[
    {
        "learn_pos_emb": true,
        "tied_weights": false,
        "embedding_dim": 64,
        "transformer_dim": 64,
        "transformer_hidden_dim": 128,
        "head_dim": 32,
        "num_head": 2,
        "num_layers": 4,
        "vocab_size": 32,
        "max_seq_len": 2048,
        "dropout_prob": 0.1,
        "attention_dropout": 0.1,
        "pooling_mode": "MEAN",
        "num_classes": 10,
        "block_size": 64,
        "batch_size": 32,
        "density": 0.04,
        "mixed_precision": true,
        "random_seed": 5,
        "task": "lra-listops"
    },
    {
        "batch_size": 128,
        "learning_rate": 0.0006,
        "warmup": 1000,
        "lr_decay": "linear",
        "weight_decay": 0,
        "eval_frequency": 500,
        "num_train_steps": 10000,
        "num_init_steps": 1000,
        "num_eval_steps": 62,
        "num_dense_train_steps": 1000,
        "patience": 10,
        "attn_loss_scale": 0.01
    }
]
attn_mask compile
attn_mask compile
attn_mask compile
attn_mask compile
DataParallel(
  (module): ModelForSC(
    (model): Model(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(32, 64)
        (position_embeddings): Embedding(2048, 64)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_0): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_1): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_2): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_3): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (seq_classifer): SCHead(
      (mlpblock): Sequential(
        (0): Linear(in_features=64, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=10, bias=True)
      )
    )
  )
)
parameter_size: [torch.Size([32, 64]), torch.Size([2048, 64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]
num_parameter: 280842
Loaded ../Skyformer/data/lra_processed/lra-listops.train.pickle... size=96000
Loaded ../Skyformer/data/lra_processed/lra-listops.dev.pickle... size=2000
Loaded ../Skyformer/data/lra_processed/lra-listops.test.pickle... size=2000
accumu_steps=1
[tensor([37.8870, 37.8870, 37.8870, 37.8870], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([37.5817, 37.5817, 37.5817, 37.5817], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([35.2380, 35.2380, 35.2380, 35.2381], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([35.5831, 35.5832, 35.5831, 35.5832], device='cuda:0',
       grad_fn=<GatherBackward>)]
best model saved: step =  499 dev accu =  tensor(0.1594, device='cuda:0')

Validation Results
Global Steps: 499
Valid Loss: 2.25854
Valid Accuracy: 0.15940
time stamp: 828.8356730937958
[tensor([10.5474, 10.5474, 10.5474, 10.5474], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([10.4545, 10.4545, 10.4545, 10.4545], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([9.7318, 9.7317, 9.7318, 9.7317], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([9.7712, 9.7712, 9.7712, 9.7713], device='cuda:0',
       grad_fn=<GatherBackward>)]
best model saved: step =  999 dev accu =  tensor(0.3575, device='cuda:0')

Validation Results
Global Steps: 999
Valid Loss: 1.72809
Valid Accuracy: 0.35748
time stamp: 1660.8429582118988
module.model.transformer_0.mha.pattern saved
module.model.transformer_1.mha.pattern saved
module.model.transformer_2.mha.pattern saved
module.model.transformer_3.mha.pattern saved
./pickle/lra-listops/module.model.transformer_0.mha.pattern.pickle
tensor([891, 528, 429, 297, 462,  99, 495, 990, 594, 264, 759, 165, 693, 561,
        396, 858, 924, 792, 198, 627, 363, 475, 878, 660, 330, 825, 132, 539,
        880, 530, 592,  66, 347, 874, 562, 593, 439, 749,   0, 957],
       device='cuda:0')
tensor(40, device='cuda:0')
block_attn_mask compile
./pickle/lra-listops/module.model.transformer_1.mha.pattern.pickle
tensor([ 561,  858,  429,  957,  924,  495,  693,  627,   99,  297,  165,  132,
        1023,  792,  330,  594,  433,  557,  198,  264,   33,  231,    0,  497,
         559,  462,  825,  860,  922,  666,  852,  396,  726,  145,  548,  891,
         759,  575, 1009,  363], device='cuda:0')
tensor(40, device='cuda:0')
block_attn_mask compile
./pickle/lra-listops/module.model.transformer_2.mha.pattern.pickle
tensor([ 528,  297,  132,  264,  957,  726,  495,  462,  561,   66, 1023,  759,
         990,  693,  231,  891,  363,  165,  198,  594,  858,  792,  660,  477,
         942,  734,  982,  924,  627,  958,  989,  733,  950,  496,  527,   72,
         258,  429,  825,  305], device='cuda:0')
tensor(40, device='cuda:0')
block_attn_mask compile
./pickle/lra-listops/module.model.transformer_3.mha.pattern.pickle
tensor([ 495,  396,  858,  165,   99,  330,  198,   66,  891,  792,  924,  825,
         363,  561,  528,  726,  429,  957,  660, 1023,  627,  297,    0,  990,
         759,  264,  111,  483,  462,  693,  231,  207,  486,   79,  482,  504,
         783,  431,  493,  132], device='cuda:0')
tensor(40, device='cuda:0')
block_attn_mask compile
total pattern searching time (s): 0.12412500381469727
[]
best model saved: step =  1499 dev accu =  tensor(0.3623, device='cuda:0')

Validation Results
Global Steps: 1499
Valid Loss: 1.82879
Valid Accuracy: 0.36227
time stamp: 2109.062999486923
[]

Validation Results
Global Steps: 1999
Valid Loss: 1.76076
Valid Accuracy: 0.35534
time stamp: 2154.284937620163
[]

Validation Results
Global Steps: 2499
Valid Loss: 1.73734
Valid Accuracy: 0.35799
time stamp: 2199.602166891098
[]

Validation Results
Global Steps: 2999
Valid Loss: 1.71118
Valid Accuracy: 0.35370
time stamp: 2245.4396057128906
[]

Validation Results
Global Steps: 3499
Valid Loss: 1.71414
Valid Accuracy: 0.35748
time stamp: 2291.2901406288147
[]

Validation Results
Global Steps: 3999
Valid Loss: 1.69340
Valid Accuracy: 0.35496
time stamp: 2337.0433597564697
[]
best model saved: step =  4499 dev accu =  tensor(0.3682, device='cuda:0')

Validation Results
Global Steps: 4499
Valid Loss: 1.70027
Valid Accuracy: 0.36820
time stamp: 2382.405722618103
[]

Validation Results
Global Steps: 4999
Valid Loss: 1.69356
Valid Accuracy: 0.36542
time stamp: 2428.0197656154633
[]
best model saved: step =  5499 dev accu =  tensor(0.3684, device='cuda:0')

Validation Results
Global Steps: 5499
Valid Loss: 1.68902
Valid Accuracy: 0.36845
time stamp: 2471.9387154579163
[]

Validation Results
Global Steps: 5999
Valid Loss: 1.69905
Valid Accuracy: 0.36467
time stamp: 2515.5977506637573
[]
best model saved: step =  6499 dev accu =  tensor(0.3701, device='cuda:0')

Validation Results
Global Steps: 6499
Valid Loss: 1.68714
Valid Accuracy: 0.37009
time stamp: 2559.4594798088074
[]

Validation Results
Global Steps: 6999
Valid Loss: 1.69032
Valid Accuracy: 0.36857
time stamp: 2603.322309732437
[]

Validation Results
Global Steps: 7499
Valid Loss: 1.69661
Valid Accuracy: 0.36379
time stamp: 2646.730710506439
[]

Validation Results
Global Steps: 7999
Valid Loss: 1.69723
Valid Accuracy: 0.36505
time stamp: 2690.5801825523376
[]

Validation Results
Global Steps: 8499
Valid Loss: 1.68828
Valid Accuracy: 0.36782
time stamp: 2734.1581201553345
[]

Validation Results
Global Steps: 8999
Valid Loss: 1.69615
Valid Accuracy: 0.36782
time stamp: 2777.838142633438
[]

Validation Results
Global Steps: 9499
Valid Loss: 1.69013
Valid Accuracy: 0.36605
time stamp: 2821.771075487137
[]

Validation Results
Global Steps: 9999
Valid Loss: 1.69424
Valid Accuracy: 0.36832
time stamp: 2865.239433526993
total training step (k): 10.0
total training time (s): 2865.240124464035
total training time (ms): 53665.67248535156
peak memory usage (MB): 12399
allocated memory usage (MB): 87424807
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    7572 KB |   12399 MB |   85375 GB |   85375 GB |
|       from large pool |    3145 KB |   12391 MB |   84871 GB |   84871 GB |
|       from small pool |    4427 KB |      13 MB |     504 GB |     504 GB |
|---------------------------------------------------------------------------|
| Active memory         |    7572 KB |   12399 MB |   85375 GB |   85375 GB |
|       from large pool |    3145 KB |   12391 MB |   84871 GB |   84871 GB |
|       from small pool |    4427 KB |      13 MB |     504 GB |     504 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   13530 MB |   13530 MB |   13530 MB |       0 B  |
|       from large pool |   13516 MB |   13516 MB |   13516 MB |       0 B  |
|       from small pool |      14 MB |      14 MB |      14 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1040 MB |    2041 MB |   42979 GB |   42978 GB |
|       from large pool |    1036 MB |    2036 MB |   42424 GB |   42423 GB |
|       from small pool |       3 MB |       7 MB |     555 GB |     555 GB |
|---------------------------------------------------------------------------|
| Allocations           |     246    |     407    |    8413 K  |    8413 K  |
|       from large pool |       2    |      93    |    3521 K  |    3521 K  |
|       from small pool |     244    |     381    |    4891 K  |    4891 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     246    |     407    |    8413 K  |    8413 K  |
|       from large pool |       2    |      93    |    3521 K  |    3521 K  |
|       from small pool |     244    |     381    |    4891 K  |    4891 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      32    |      32    |      32    |       0    |
|       from large pool |      25    |      25    |      25    |       0    |
|       from small pool |       7    |       7    |       7    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      11    |      31    |    4311 K  |    4311 K  |
|       from large pool |       3    |      18    |    1864 K  |    1864 K  |
|       from small pool |       8    |      20    |    2447 K  |    2447 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

loading the best model from: ./checkpoints/checkpoints-5/lra-listops/learnable.model
Evaluation Results
Loss: 1.66900
Accuracy: 0.37604
