GPU list: [0, 1, 2, 3]
[
    {
        "learn_pos_emb": true,
        "tied_weights": false,
        "embedding_dim": 64,
        "transformer_dim": 64,
        "transformer_hidden_dim": 128,
        "head_dim": 32,
        "num_head": 2,
        "num_layers": 2,
        "vocab_size": 512,
        "max_seq_len": 1024,
        "dropout_prob": 0.1,
        "attention_dropout": 0.1,
        "pooling_mode": "MEAN",
        "num_classes": 14,
        "block_size": 32,
        "batch_size": 128,
        "density": 0.04,
        "mixed_precision": true,
        "random_seed": 5,
        "task": "lra-nature"
    },
    {
        "batch_size": 512,
        "learning_rate": 0.003,
        "warmup": 800,
        "lr_decay": "linear",
        "weight_decay": 0,
        "eval_frequency": 500,
        "num_train_steps": 10000,
        "num_init_steps": 3000,
        "num_eval_steps": 300,
        "num_dense_train_steps": 1000,
        "patience": 10,
        "attn_loss_scale": 0.01
    }
]
attn compile
attn compile
DataParallel(
  (module): ModelForSC(
    (model): Model(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(512, 64)
        (position_embeddings): Embedding(1024, 64)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_0): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=32, stride=32, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_1): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=32, stride=32, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (seq_classifer): SCHead(
      (mlpblock): Sequential(
        (0): Linear(in_features=64, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=14, bias=True)
      )
    )
  )
)
parameter_size: [torch.Size([512, 64]), torch.Size([1024, 64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([14, 128]), torch.Size([14])]
num_parameter: 177550
Loaded ../Skyformer/data/lra_processed/lra-nature.train.pickle... size=437513
Loaded ../Skyformer/data/lra_processed/lra-nature.dev.pickle... size=24426
Loaded ../Skyformer/data/lra_processed/lra-nature.test.pickle... size=24426
accumu_steps=1
module.model.transformer_0.mha.pattern saved
module.model.transformer_1.mha.pattern saved
./pickle/lra-nature/module.model.transformer_0.mha.pattern.pickle
tensor([ 990, 1023,  792,    0,  528,  363,   99,  759,  858,  891,  561,  957,
         165,  594,  660,  396,  330,  297,  132,   33,  429,  231,  495,  693,
         627,  991, 1022,  825,  767, 1015,  415, 1004,  543, 1008,  542,  976,
          11,  352,  726,  529], device='cuda:0')
tensor(40, device='cuda:0')
block_attn compile
./pickle/lra-nature/module.model.transformer_1.mha.pattern.pickle
tensor([  66,  759,  297,  825,  528,  858,  396,  231,  726,  363,  132,  891,
         660,  264,  330,  462,  495,    0,  561,  627,   99,  792,   76,  386,
         429,   71,  226,  198,  594, 1023,  990,  693,  165,  311,  745,  957,
         313,  809,   67,   98], device='cuda:0')
tensor(40, device='cuda:0')
block_attn compile
total pattern searching time (s): 0.005397796630859375
[]
best model saved: step =  499 dev accu =  tensor(0.3911, device='cuda:0')

Validation Results
Global Steps: 499
Valid Loss: 1.80814
Valid Accuracy: 0.39114
time stamp: 183.01890802383423
[]
best model saved: step =  999 dev accu =  tensor(0.4219, device='cuda:0')

Validation Results
Global Steps: 999
Valid Loss: 1.75520
Valid Accuracy: 0.42186
time stamp: 224.05368828773499
[]
best model saved: step =  1499 dev accu =  tensor(0.4642, device='cuda:0')

Validation Results
Global Steps: 1499
Valid Loss: 1.66998
Valid Accuracy: 0.46418
time stamp: 264.4068326950073
[]
best model saved: step =  1999 dev accu =  tensor(0.4811, device='cuda:0')

Validation Results
Global Steps: 1999
Valid Loss: 1.62330
Valid Accuracy: 0.48109
time stamp: 305.7860722541809
[]

Validation Results
Global Steps: 2499
Valid Loss: 1.66100
Valid Accuracy: 0.46347
time stamp: 347.16363072395325
[]

Validation Results
Global Steps: 2999
Valid Loss: 1.72357
Valid Accuracy: 0.43147
time stamp: 388.5055983066559
[]
best model saved: step =  3499 dev accu =  tensor(0.4833, device='cuda:0')

Validation Results
Global Steps: 3499
Valid Loss: 1.61069
Valid Accuracy: 0.48333
time stamp: 429.91533875465393
[]

Validation Results
Global Steps: 3999
Valid Loss: 1.62755
Valid Accuracy: 0.46564
time stamp: 470.8003532886505
[]
best model saved: step =  4499 dev accu =  tensor(0.5025, device='cuda:0')

Validation Results
Global Steps: 4499
Valid Loss: 1.56072
Valid Accuracy: 0.50246
time stamp: 512.1190071105957
[]

Validation Results
Global Steps: 4999
Valid Loss: 1.61201
Valid Accuracy: 0.47423
time stamp: 553.1631503105164
[]

Validation Results
Global Steps: 5499
Valid Loss: 1.61323
Valid Accuracy: 0.47967
time stamp: 595.0108587741852
[]

Validation Results
Global Steps: 5999
Valid Loss: 1.58471
Valid Accuracy: 0.48974
time stamp: 636.2329380512238
[]

Validation Results
Global Steps: 6499
Valid Loss: 1.57338
Valid Accuracy: 0.49733
time stamp: 677.5061297416687
[]
best model saved: step =  6999 dev accu =  tensor(0.5052, device='cuda:0')

Validation Results
Global Steps: 6999
Valid Loss: 1.55753
Valid Accuracy: 0.50516
time stamp: 719.149961233139
[]

Validation Results
Global Steps: 7499
Valid Loss: 1.61605
Valid Accuracy: 0.48326
time stamp: 760.567804813385
[]

Validation Results
Global Steps: 7999
Valid Loss: 1.56401
Valid Accuracy: 0.49956
time stamp: 801.7743306159973
[]
best model saved: step =  8499 dev accu =  tensor(0.5141, device='cuda:0')

Validation Results
Global Steps: 8499
Valid Loss: 1.53105
Valid Accuracy: 0.51407
time stamp: 843.4953465461731
[]
best model saved: step =  8999 dev accu =  tensor(0.5186, device='cuda:0')

Validation Results
Global Steps: 8999
Valid Loss: 1.53570
Valid Accuracy: 0.51863
time stamp: 884.4975740909576
[]

Validation Results
Global Steps: 9499
Valid Loss: 1.59529
Valid Accuracy: 0.48189
time stamp: 925.8820903301239
[]

Validation Results
Global Steps: 9999
Valid Loss: 1.54678
Valid Accuracy: 0.50508
time stamp: 967.1434459686279
total training step (k): 10.0
total training time (s): 967.1443083286285
total training time (ms): 172881.919921875
peak memory usage (MB): 8036
allocated memory usage (MB): 61124496
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   10389 KB |    8036 MB |   59691 GB |   59691 GB |
|       from large pool |    6144 KB |    8030 MB |   59201 GB |   59201 GB |
|       from small pool |    4245 KB |      15 MB |     490 GB |     490 GB |
|---------------------------------------------------------------------------|
| Active memory         |   10389 KB |    8036 MB |   59691 GB |   59691 GB |
|       from large pool |    6144 KB |    8030 MB |   59201 GB |   59201 GB |
|       from small pool |    4245 KB |      15 MB |     490 GB |     490 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    8614 MB |    8614 MB |    8614 MB |       0 B  |
|       from large pool |    8596 MB |    8596 MB |    8596 MB |       0 B  |
|       from small pool |      18 MB |      18 MB |      18 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   16235 KB |    1109 MB |   51679 GB |   51679 GB |
|       from large pool |   14336 KB |    1106 MB |   51153 GB |   51153 GB |
|       from small pool |    1899 KB |       4 MB |     525 GB |     525 GB |
|---------------------------------------------------------------------------|
| Allocations           |     228    |     574    |    6178 K  |    6178 K  |
|       from large pool |       2    |      49    |    2145 K  |    2145 K  |
|       from small pool |     226    |     565    |    4033 K  |    4032 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     228    |     574    |    6178 K  |    6178 K  |
|       from large pool |       2    |      49    |    2145 K  |    2145 K  |
|       from small pool |     226    |     565    |    4033 K  |    4032 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      31    |      31    |      31    |       0    |
|       from large pool |      22    |      22    |      22    |       0    |
|       from small pool |       9    |       9    |       9    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |      25    |    2774 K  |    2774 K  |
|       from large pool |       2    |      15    |     769 K  |     769 K  |
|       from small pool |       6    |      16    |    2004 K  |    2004 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

loading the best model from: ./checkpoints/checkpoints-5/lra-nature/learnable.model
Evaluation Results
Loss: 1.53650
Accuracy: 0.51858
