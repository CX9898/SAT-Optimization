GPU list: [0, 1, 2, 3]
[
    {
        "learn_pos_emb": true,
        "tied_weights": false,
        "embedding_dim": 64,
        "transformer_dim": 64,
        "transformer_hidden_dim": 128,
        "head_dim": 32,
        "num_head": 2,
        "num_layers": 2,
        "vocab_size": 512,
        "max_seq_len": 4096,
        "dropout_prob": 0.1,
        "attention_dropout": 0.1,
        "pooling_mode": "MEAN",
        "block_size": 64,
        "num_classes": 2,
        "batch_size": 8,
        "density": 0.021,
        "mixed_precision": true,
        "random_seed": 3,
        "task": "lra-retrieval"
    },
    {
        "batch_size": 32,
        "learning_rate": 0.0005,
        "warmup": 800,
        "lr_decay": "linear",
        "weight_decay": 0,
        "eval_frequency": 200,
        "num_train_steps": 10000,
        "num_init_steps": 3000,
        "num_eval_steps": 300,
        "num_dense_train_steps": 1000,
        "patience": 10,
        "attn_loss_scale": 0.01
    }
]
attn_mask compile
attn_mask compile
DataParallel(
  (module): ModelForSCDual(
    (model): Model(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(512, 64)
        (position_embeddings): Embedding(4096, 64)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_0): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_1): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (seq_classifer): SCHeadDual(
      (mlpblock): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=2, bias=True)
      )
    )
  )
)
parameter_size: [torch.Size([512, 64]), torch.Size([4096, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([64, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 256]), torch.Size([128]), torch.Size([2, 128]), torch.Size([2])]
num_parameter: 399234
Loaded ../Skyformer/data/lra_processed/lra-retrieval.train.pickle... size=147086
Loaded ../Skyformer/data/lra_processed/lra-retrieval.dev.pickle... size=18090
Loaded ../Skyformer/data/lra_processed/lra-retrieval.test.pickle... size=17437
accumu_steps=1
module.model.transformer_0.mha.pattern saved
module.model.transformer_1.mha.pattern saved
./pickle/lra-retrieval/module.model.transformer_0.mha.pattern.pickle
tensor([ 130, 1950, 1820, 3055, 3705, 1560, 1885, 1105, 4030, 1170,  455,  152,
        1538,  158, 1922,  185, 3650,  190, 3970,  145, 1090, 1977, 3678,  175,
        3010, 1495, 1821, 1884,  156, 1794, 2145, 1116, 1809,  146, 1154,  157,
        1858, 2795,  325, 1182, 1938, 3315, 1135, 3025, 1145, 3665, 1849, 3676,
         260, 3445, 1886, 1949, 3065, 3695, 1822, 1948,  585,  181, 3394, 2990,
        1903, 3037,  284, 1796, 3640, 1502, 1943, 1593, 3672, 1755, 1040, 3510,
        1117, 1873,  133,  322,  159, 1986,  171, 2754, 1566, 1944, 1967, 3038,
        2470, 2925], device='cuda:0')
tensor(86, device='cuda:0')
block_attn_mask compile
./pickle/lra-retrieval/module.model.transformer_1.mha.pattern.pickle
tensor([3640, 1950,  650, 1690, 2600, 1105,    0, 3250,  780, 1720, 3610, 3120,
        1560, 1960, 2590,   30, 1920,  130, 1694, 1946,  158, 1922,  168, 2562,
        1976, 3614,  798, 1932, 4095, 2990, 4030, 1885,  696, 3594,  666, 1674,
         910, 1704, 2586, 1040, 1896, 2589,  222, 1923,  195, 2340, 3770, 2925,
         808, 2572,  248, 3587, 1886, 1949, 2535,  657, 1098,   56, 3584, 2665,
        1712, 3098, 1956, 2334, 1170, 1968, 3102, 3128, 3632, 1054, 1936,   40,
        2560,   48, 3072,  926, 1934, 1365, 1709, 2906, 2616, 3624, 1755, 1978,
        3742,   26], device='cuda:0')
tensor(86, device='cuda:0')
block_attn_mask compile
total pattern searching time (s): 0.11995506286621094
