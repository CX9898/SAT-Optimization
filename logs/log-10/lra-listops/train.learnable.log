GPU list: [0, 1, 2, 3]
[
    {
        "learn_pos_emb": true,
        "tied_weights": false,
        "embedding_dim": 64,
        "transformer_dim": 64,
        "transformer_hidden_dim": 128,
        "head_dim": 32,
        "num_head": 2,
        "num_layers": 4,
        "vocab_size": 32,
        "max_seq_len": 2048,
        "dropout_prob": 0.1,
        "attention_dropout": 0.1,
        "pooling_mode": "MEAN",
        "num_classes": 10,
        "block_size": 64,
        "batch_size": 32,
        "density": 0.04,
        "mixed_precision": true,
        "random_seed": 10,
        "task": "lra-listops"
    },
    {
        "batch_size": 128,
        "learning_rate": 0.0006,
        "warmup": 1000,
        "lr_decay": "linear",
        "weight_decay": 0,
        "eval_frequency": 500,
        "num_train_steps": 10000,
        "num_init_steps": 1000,
        "num_eval_steps": 62,
        "num_dense_train_steps": 1000,
        "patience": 10,
        "attn_loss_scale": 0.01
    }
]
attn_mask compile
attn_mask compile
attn_mask compile
attn_mask compile
DataParallel(
  (module): ModelForSC(
    (model): Model(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(32, 64)
        (position_embeddings): Embedding(2048, 64)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer_0): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_1): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_2): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (transformer_3): TransformerLayer(
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mha): Attention(
          (W_q): Linear(in_features=64, out_features=64, bias=True)
          (W_k): Linear(in_features=64, out_features=64, bias=True)
          (W_v): Linear(in_features=64, out_features=64, bias=True)
          (avg_pool): AvgPool2d(kernel_size=64, stride=64, padding=0)
          (MSEloss): MSELoss()
          (attn): CUDAMaskAttention()
          (ff): Linear(in_features=64, out_features=64, bias=True)
        )
        (dropout1): Dropout(p=0.1, inplace=False)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlpblock): Sequential(
          (0): Linear(in_features=64, out_features=128, bias=True)
          (1): GELU(approximate='none')
          (2): Dropout(p=0.1, inplace=False)
          (3): Linear(in_features=128, out_features=64, bias=True)
          (4): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (seq_classifer): SCHead(
      (mlpblock): Sequential(
        (0): Linear(in_features=64, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=10, bias=True)
      )
    )
  )
)
parameter_size: [torch.Size([32, 64]), torch.Size([2048, 64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([32, 32]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]
num_parameter: 280842
Loaded ../Skyformer/data/lra_processed/lra-listops.train.pickle... size=96000
Loaded ../Skyformer/data/lra_processed/lra-listops.dev.pickle... size=2000
Loaded ../Skyformer/data/lra_processed/lra-listops.test.pickle... size=2000
accumu_steps=1
[tensor([39.6558, 39.6558, 39.6558, 39.6558], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([36.2254, 36.2254, 36.2254, 36.2254], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([35.6793, 35.6794, 35.6793, 35.6794], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([34.5652, 34.5652, 34.5652, 34.5652], device='cuda:0',
       grad_fn=<GatherBackward>)]
best model saved: step =  499 dev accu =  tensor(0.1717, device='cuda:0')

Validation Results
Global Steps: 499
Valid Loss: 2.25765
Valid Accuracy: 0.17175
time stamp: 828.0036833286285
[tensor([11.1203, 11.1203, 11.1203, 11.1203], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([10.0485, 10.0485, 10.0485, 10.0485], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([9.7920, 9.7920, 9.7920, 9.7920], device='cuda:0',
       grad_fn=<GatherBackward>), tensor([9.5028, 9.5028, 9.5028, 9.5028], device='cuda:0',
       grad_fn=<GatherBackward>)]
best model saved: step =  999 dev accu =  tensor(0.3589, device='cuda:0')

Validation Results
Global Steps: 999
Valid Loss: 1.72005
Valid Accuracy: 0.35887
time stamp: 1659.5973989963531
module.model.transformer_0.mha.pattern saved
module.model.transformer_1.mha.pattern saved
module.model.transformer_2.mha.pattern saved
module.model.transformer_3.mha.pattern saved
./pickle/lra-listops/module.model.transformer_0.mha.pattern.pickle
tensor([ 792,  462,  990,  726,  759,  858,  825,  693,  924,  165,    0,  660,
         330,  594,  957,  132,  495,  561,   99,  627,  396,  798,  984,  363,
         796,  920,  862,  986,   66,  297,  198,  695,  757,   33,  794,  856,
         694,  725, 1023,  891], device='cuda:0')
tensor(40, device='cuda:0')
block_attn_mask compile
./pickle/lra-listops/module.model.transformer_1.mha.pattern.pickle
tensor([ 330,  891,   99,  594,  363,  792,  693,  726,  462,  528,  347,  874,
         123,  867,  957,  198,  660,  297,  561,  429,   33,    0,  466,  590,
         495,  858,  379,  875,  667,  884,  107,  355,  338,  586,  165,  342,
         714,  112,  515, 1023], device='cuda:0')
tensor(40, device='cuda:0')
block_attn_mask compile
./pickle/lra-listops/module.model.transformer_2.mha.pattern.pickle
tensor([ 759,  429,   99,  561,  231,  726,  396,  627,  264,  495,  891,  594,
         792,  825,  660,  858,  693,  957,  198,  132,   66,  363,  567,  753,
          33,  563,  625,  528, 1023,  165,  924,  990,  297,  330,    0,  115,
         611,  462,  695,  757], device='cuda:0')
tensor(40, device='cuda:0')
block_attn_mask compile
./pickle/lra-listops/module.model.transformer_3.mha.pattern.pickle
tensor([ 363,  462,  660,  330,   99,  264,   33,  528,   66,  165,  858,  366,
         459,  495,  198,  693,  297,  594, 1023,  627,    0,  231,  132,  561,
         107,  355,  368,  523,   43,  353,  429,  990,  171,  357,  331,  362,
         396,   75,  354,   11], device='cuda:0')
tensor(40, device='cuda:0')
block_attn_mask compile
total pattern searching time (s): 0.12198972702026367
[]

Validation Results
Global Steps: 1499
Valid Loss: 1.74399
Valid Accuracy: 0.35270
time stamp: 1763.8825907707214
[]
best model saved: step =  1999 dev accu =  tensor(0.3655, device='cuda:0')

Validation Results
Global Steps: 1999
Valid Loss: 1.71514
Valid Accuracy: 0.36555
time stamp: 1809.607183456421
[]

Validation Results
Global Steps: 2499
Valid Loss: 1.68714
Valid Accuracy: 0.36026
time stamp: 1855.4651238918304
[]

Validation Results
Global Steps: 2999
Valid Loss: 1.68708
Valid Accuracy: 0.36353
time stamp: 1901.1851389408112
[]
best model saved: step =  3499 dev accu =  tensor(0.3720, device='cuda:0')

Validation Results
Global Steps: 3499
Valid Loss: 1.66734
Valid Accuracy: 0.37198
time stamp: 1946.901537179947
[]

Validation Results
Global Steps: 3999
Valid Loss: 1.66784
Valid Accuracy: 0.36605
time stamp: 1991.9760389328003
[]

Validation Results
Global Steps: 4499
Valid Loss: 1.68284
Valid Accuracy: 0.35975
time stamp: 2035.93984913826
[]

Validation Results
Global Steps: 4999
Valid Loss: 1.67787
Valid Accuracy: 0.36202
time stamp: 2079.6166059970856
[]

Validation Results
Global Steps: 5499
Valid Loss: 1.66688
Valid Accuracy: 0.36631
time stamp: 2123.352910041809
[]

Validation Results
Global Steps: 5999
Valid Loss: 1.67613
Valid Accuracy: 0.36290
time stamp: 2167.652390241623
[]

Validation Results
Global Steps: 6499
Valid Loss: 1.67194
Valid Accuracy: 0.36341
time stamp: 2211.730567216873
[]

Validation Results
Global Steps: 6999
Valid Loss: 1.67519
Valid Accuracy: 0.36391
time stamp: 2254.8568081855774
[]

Validation Results
Global Steps: 7499
Valid Loss: 1.67439
Valid Accuracy: 0.36316
time stamp: 2298.7088832855225
[]

Validation Results
Global Steps: 7999
Valid Loss: 1.67418
Valid Accuracy: 0.36429
time stamp: 2342.3177621364594
[]

Validation Results
Global Steps: 8499
Valid Loss: 1.67171
Valid Accuracy: 0.36353
time stamp: 2386.824024915695
[]

Validation Results
Global Steps: 8999
Valid Loss: 1.67441
Valid Accuracy: 0.36580
time stamp: 2430.3487434387207
[]

Validation Results
Global Steps: 9499
Valid Loss: 1.67694
Valid Accuracy: 0.36089
time stamp: 2474.286771297455
[]

Validation Results
Global Steps: 9999
Valid Loss: 1.67696
Valid Accuracy: 0.36341
time stamp: 2518.041244506836
total training step (k): 10.0
total training time (s): 2518.0421063899994
total training time (ms): 53645.344970703125
peak memory usage (MB): 12399
allocated memory usage (MB): 81284990
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    7572 KB |   12399 MB |   79379 GB |   79379 GB |
|       from large pool |    3145 KB |   12391 MB |   78878 GB |   78878 GB |
|       from small pool |    4427 KB |      13 MB |     501 GB |     501 GB |
|---------------------------------------------------------------------------|
| Active memory         |    7572 KB |   12399 MB |   79379 GB |   79379 GB |
|       from large pool |    3145 KB |   12391 MB |   78878 GB |   78878 GB |
|       from small pool |    4427 KB |      13 MB |     501 GB |     501 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   13530 MB |   13530 MB |   13530 MB |       0 B  |
|       from large pool |   13516 MB |   13516 MB |   13516 MB |       0 B  |
|       from small pool |      14 MB |      14 MB |      14 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    1040 MB |    2001 MB |   43098 GB |   43097 GB |
|       from large pool |    1036 MB |    1996 MB |   42555 GB |   42554 GB |
|       from small pool |       3 MB |       7 MB |     542 GB |     542 GB |
|---------------------------------------------------------------------------|
| Allocations           |     246    |     407    |    8240 K  |    8240 K  |
|       from large pool |       2    |      93    |    3520 K  |    3520 K  |
|       from small pool |     244    |     381    |    4720 K  |    4720 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     246    |     407    |    8240 K  |    8240 K  |
|       from large pool |       2    |      93    |    3520 K  |    3520 K  |
|       from small pool |     244    |     381    |    4720 K  |    4720 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      32    |      32    |      32    |       0    |
|       from large pool |      25    |      25    |      25    |       0    |
|       from small pool |       7    |       7    |       7    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      13    |      33    |    4144 K  |    4144 K  |
|       from large pool |       3    |      18    |    1863 K  |    1863 K  |
|       from small pool |      10    |      24    |    2281 K  |    2281 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

loading the best model from: ./checkpoints/checkpoints-10/lra-listops/learnable.model
Evaluation Results
Loss: 1.64792
Accuracy: 0.37917
